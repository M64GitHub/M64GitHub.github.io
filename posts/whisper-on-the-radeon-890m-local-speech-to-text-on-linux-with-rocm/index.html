<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="keywords" content="amd, rocm, whisper, speech-to-text, linux, radeon-890m, strix-point, local-ai">
  <title>Whisper on the Radeon 890M: Local Speech-to-Text on Linux with ROCm</title>
  <meta property="og:image" content="https://m64github.github.io/posts/whisper-on-the-radeon-890m-local-speech-to-text-on-linux-with-rocm/images/preview.png">
  <meta property="og:title" content="Whisper on the Radeon 890M: Local Speech-to-Text on Linux with ROCm">
  <meta property="og:description" content="ROCm finally supports Strix Point - and I had the perfect use case: replacing Wispr Flow with 500 lines of Python.">
  <link rel="stylesheet" href="../article-style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
</head>
<body>
  <div class="container">
    <aside class="sidebar">
      <nav class="table-of-contents">
<h2>Contents</h2>
<ul>
  <li ><a href="#switching-to-linux-for-real-performance">Switching to Linux for Real Performance</a></li>
  <li ><a href="#why-existing-linux-solutions-didn-t-work">Why Existing Linux Solutions Didn't Work</a></li>
  <li ><a href="#rocm-finally-supports-strix-point">ROCm Finally Supports Strix Point</a></li>
  <li ><a href="#building-a-local-whisper-app">Building a Local Whisper App</a></li>
  <li ><a href="#performance">Performance</a></li>
  <li ><a href="#try-it-yourself">Try It Yourself</a></li>
  <li ><a href="#final-notes">Final Notes</a></li>
</ul>
</nav>

    </aside>

    <main class="article-content">
      <div class="article-tags">
    <span class="tag">#amd</span> <span class="tag">#rocm</span> <span class="tag">#whisper</span> <span class="tag">#speech-to-text</span> <span class="tag">#linux</span> <span class="tag">#radeon-890m</span> <span class="tag">#strix-point</span> <span class="tag">#local-ai</span>
  </div>
<h1 id="whisper-on-the-radeon-890m-local-speech-to-text-on-linux-with-rocm">Whisper on the Radeon 890M: Local Speech-to-Text on Linux with ROCm</h1>
<p><strong>ROCm finally supports Strix Point - and I had the perfect use case: replacing Wispr Flow with 500 lines of Python.</strong></p>
<div class="article-byline">M. Schallner, 2025.12.29</div>
<p>AMD recently added ROCm support for Strix Point GPUs. That means the integrated Radeon 890M (gfx1150) can now run GPU-accelerated ML workloads on Linux. I used this to run OpenAI&#39;s Whisper locally and build a small, entirely offline voice-to-text app that replaces Wispr Flow in about 500 lines of Python.</p>
<h2 id="switching-to-linux-for-real-performance">Switching to Linux for Real Performance</h2>
<p>Up until this point, my main machine had been a MacBook with Apple Silicon (M4) - fast, reliable, and a great fit for my day-to-day work and development projects. But for what I wanted to do next, that wasn&#39;t quite enough.</p>
<p>I needed accurate, undistorted performance measurements for low-level network I/O work on <code>Linux</code> - real io_uring measurements without VMs or compatibility layers in the way.</p>
<p>So I finally did something I&#39;d secretly wanted for a while: get a proper Linux laptop.</p>
<p>The result was a brand-new Tuxedo: an AMD Ryzen AI 9 HX 370 with 96 GB RAM, running Ubuntu 24.04. It turned out to be a beast. Everything worked out of the box - my Zig toolchain, Neovim builds in 15 seconds, all my usual workflows. Compile times rivaled the M4.</p>
<p>The transition felt almost boringly smooth. Everything carried over without friction. I didn&#39;t feel like I was giving anything up.</p>
<p>At least not at first.</p>
<p>One thing I hadn&#39;t thought about at all was voice-to-text. On macOS, <code>Wispr Flow</code> had quietly become part of my daily workflow - quick notes, emails, documentation drafts. When I tried to replicate that on Linux, things got complicated fast.</p>
<h2 id="why-existing-linux-solutions-didn-39-t-work">Why Existing Linux Solutions Didn&#39;t Work</h2>
<p>My search for a Linux replacement was kind of frustrating. The options I found were a combination of:</p>
<ul>
<li>Difficult to install with complex dependencies</li>
<li>Didn&#39;t work out of the box</li>
<li>Required cloud APIs (defeating the &quot;local&quot; purpose I had in mind)</li>
<li><strong>Didn&#39;t support my hardware</strong></li>
</ul>
<p>I started researching the underlying technology - OpenAI&#39;s Whisper models. The question became: <em>Can I just run this myself?</em></p>
<p>How do I chain microphone input to the model and display results? And what about GPU acceleration? Running Whisper on CPU is painfully slow.</p>
<h2 id="rocm-finally-supports-strix-point">ROCm Finally Supports Strix Point</h2>
<p>Here&#39;s where timing played in my favor. So I have an AMD Radeon 890M (integrated graphics on the Ryzen AI 9 HX 370) - that&#39;s the new Strix Point architecture, internally called gfx1150.</p>
<p>When I first searched for solutions with ROCm support, everything I found was outdated. No mentions of gfx1150 anywhere. I was ready to accept CPU-only inference.</p>
<p>But I did not want to accept this so easily, and by diging deeper, I discovered that AMD had <em>just</em> released ROCm support for Strix Point. The stars aligned - I got my laptop right around the time my GPU became a first-class citizen in the ROCm ecosystem!</p>
<h2 id="building-a-local-whisper-app">Building a Local Whisper App</h2>
<p>With GPU acceleration now possible, I sat down to build something. The implementation was surprisingly fast - especially for someone who doesn&#39;t do much Python. I expected quite some low-level wrangling. Instead, I had a working prototype within an hour.</p>
<p>Here&#39;s what I used:</p>
<p><strong>FastAPI</strong> for the backend. I&#39;d never used it before, but it&#39;s remarkably clean. Two endpoints: one to serve the HTML page, one POST endpoint for transcription. That&#39;s it.</p>
<p><strong>PyTorch + Whisper</strong> for inference. This was the real surprise. Loading the model and running inference is almost trivial:</p>
<pre><code class="hljs language-python">device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>
model = whisper.load_model(<span class="hljs-string">&quot;medium&quot;</span>, device=device)

<span class="hljs-comment"># Later, in the endpoint:</span>
result = model.transcribe(audio_path)
<span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;text&quot;</span>: result[<span class="hljs-string">&quot;text&quot;</span>]}
</code></pre><p>No complex setup. No manual tensor management. PyTorch&#39;s ROCm integration means I just say <code>&quot;cuda&quot;</code> and it routes to my AMD GPU transparently.</p>
<p><strong>Browser APIs</strong> for the frontend. The MediaRecorder API captures audio from the microphone, the Web Audio API provides real-time frequency data for visualization. The flow:</p>
<ol>
<li>User holds button → <code>navigator.mediaDevices.getUserMedia({ audio: true })</code></li>
<li>Audio streams into MediaRecorder, accumulating chunks</li>
<li>Simultaneously, an AnalyserNode feeds frequency data to a canvas for the live visualizer</li>
<li>User releases → audio blob sent as FormData POST to <code>/transcribe</code></li>
<li>Backend saves to temp file, runs <code>model.transcribe()</code>, returns JSON</li>
<li>Frontend displays result, auto-selects text for easy copy</li>
</ol>
<p><strong>The visualizer</strong> was a fun addition. 64 frequency bars, mirrored around the center, with a cyan-to-magenta gradient and glow effects. The Web Audio API&#39;s <code>getByteFrequencyData()</code> makes this almost too easy - you get an array of frequency amplitudes, map them to bar heights, done. Added some CSS scanlines and a vignette for that CRT aesthetic - most of the styling came from earlier projects.</p>
<p>The entire app is a single Python file - backend, HTML, CSS, and JavaScript all inline. Around 500 lines total.</p>
<p>The result:</p>
<figure class="article-image">
  <div class="image-placeholder">
    <img src="images/screenshot-ui.png" alt="Screenshot UI" onerror="this.parentElement.innerHTML='<div class=placeholder-notice>Image: Screenshot UI<br><small>Place image at: arender/output/whisper-on-the-radeon-890m-local-speech-to-text-on-linux-with-rocm/images/screenshot-ui.png</small></div>'">
  </div>
  <figcaption>Screenshot UI</figcaption>
</figure>
<p>Press and hold to record, release to transcribe. One-click copy. That&#39;s it.</p>
<p>Starting the app shows the GPU is properly detected:</p>
<pre><code>❯ python App.py
Loading Whisper model on cuda...
GPU: AMD Radeon 890M Graphics
Model loaded!
INFO:     Started server process [60192]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
</code></pre><figure class="article-image">
  <div class="image-placeholder">
    <img src="images/screenshot-recording.png" alt="Screenshot Recording" onerror="this.parentElement.innerHTML='<div class=placeholder-notice>Image: Screenshot Recording<br><small>Place image at: arender/output/whisper-on-the-radeon-890m-local-speech-to-text-on-linux-with-rocm/images/screenshot-recording.png</small></div>'">
  </div>
  <figcaption>Screenshot Recording</figcaption>
</figure>
<h2 id="performance">Performance</h2>
<p>Here&#39;s what the Radeon 890M delivers with Whisper&#39;s medium model (769M parameters), and a measurement with the small version for comparison:</p>
<table>
<thead>
<tr>
<th>Audio Duration</th>
<th>Processing Time</th>
<th>Model</th>
</tr>
</thead>
<tbody><tr>
<td>5.0s</td>
<td>3.82s</td>
<td>medium</td>
</tr>
<tr>
<td>14.4s</td>
<td>5.03s</td>
<td>medium</td>
</tr>
<tr>
<td>6.0s</td>
<td>2.11s</td>
<td>small</td>
</tr>
</tbody></table>
<p>Faster than real-time transcription on an integrated GPU. Not bad at all.</p>
<h2 id="try-it-yourself">Try It Yourself</h2>
<p>If you have a recent AMD GPU (especially the new Radeon 800M series) and want local speech-to-text, here&#39;s how to get started:</p>
<pre><code class="hljs language-bash">git <span class="hljs-built_in">clone</span> https://github.com/M64GitHub/whisper-rocm.git
<span class="hljs-built_in">cd</span> whisper-rocm
python -m venv venv
<span class="hljs-built_in">source</span> venv/bin/activate

<span class="hljs-comment"># For Radeon 890M / 880M (gfx1150):</span>
pip install --index-url https://repo.amd.com/rocm/whl/gfx1150/ torch

pip install -r requirements.txt
python App.py
</code></pre><p>Open <code>http://localhost:8000</code> in your browser, and you&#39;re done.</p>
<p>The repo includes instructions for other AMD GPUs and NVIDIA cards as well.</p>
<h2 id="final-notes">Final Notes</h2>
<p>If you have one of the latest Radeon mobile GPUs and have been wondering whether local AI workloads are possible - they are. ROCm support for Strix Point is here.</p>
<p>This little project is of course not a groundbreaking invention. It&#39;s a simple app solving a simple problem. But when I was searching for this information, I found very little about running Whisper on the newest AMD integrated graphics. Hopefully this helps someone else in the same situation.</p>
<p>The code is on GitHub: <a href="https://github.com/M64GitHub/whisper-rocm">github.com/M64GitHub/whisper-rocm</a></p>
<hr>

    </main>
  </div>

  <script>
    // Smooth scrolling for TOC links
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
          target.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }
      });
    });

    // Highlight current section in TOC
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          const id = entry.target.getAttribute('id');
          document.querySelectorAll('.table-of-contents a').forEach(a => {
            a.classList.remove('active');
            if (a.getAttribute('href') === '#' + id) {
              a.classList.add('active');
            }
          });
        }
      });
    }, { rootMargin: '-100px 0px -66%' });

    document.querySelectorAll('h2[id]').forEach(h => observer.observe(h));
  </script>
</body>
</html>